# ğŸ“Š ClickHouse Ingestion & Partitioning Guide

## Production Architecture Guidance for High-Volume & Bursty Workloads (DBRE Edition)

This guide explains how to design ingestion and storage in ClickHouse so that the system remains **stable, predictable, and scalable under bursty or continuous write workloads**.

It is written from a **production DBRE perspective**:

> We donâ€™t optimize for theoretical perfect plans â€”
> we optimize for **predictability, guardrails, and bounded blast radius** when load or estimates are wrong.

---

## ğŸ§­ Goals of This Guide

This document helps engineers avoid failure patterns we repeatedly see in real systems:

* Small-part explosion & endless merge backlogs
* Hot partitions and overloaded merge queues
* Slow queries due to thousands of files per range
* Write amplification / CPU saturation
* Insert throttling & ingestion stalls
* Out-of-control storage growth

It also defines **what we expect from new services onboarding to the platform**.

---

# 1ï¸âƒ£ Partitioning â€” The Most Important Design Decision

### âŒ Anti-pattern: Yearly partitions

```sql
PARTITION BY toYear(date)
```

Why this fails in production:

* All activity for a year ends up in one partition
* Merges occur **per partition** â†’ very large jobs
* Backfills & late data amplify write pressure
* Drops / retention operations become coarse-grained
* Operational blast radius becomes **cluster-scale**

---

### ğŸ§  Partitioning Rules We Enforce

| Principle                                            | Reason                   |
| ---------------------------------------------------- | ------------------------ |
| Partitions must align to query time ranges           | Enables pruning          |
| Hot data must live in **small, rotating partitions** | Reduces merge contention |
| Drop operations must be scoped                       | Safer retention          |
| Partition strategy must support 10Ã— growth           | Future-proofing          |

---

### ğŸ¯ Recommended Defaults (Platform Baseline)

| Workload                            | Partition Strategy           |
| ----------------------------------- | ---------------------------- |
| Analytics or rolling window queries | `toYYYYMM()` (monthly)       |
| Short-range time analytics          | weekly partitions            |
| Very high ingest / real-time        | daily partitions (carefully) |

> Monthly is the **sweet spot** for most read-heavy workloads.

---

# 2ï¸âƒ£ Part Size â€” The Stability Envelope

| Metric | Minimum | Optimal   | Upper Risk |
| ------ | ------- | --------- | ---------- |
| Rows   | 100K    | 1â€“10M     | 100M       |
| Size   | 10MB    | 100MBâ€“1GB | 10GB       |

Too many tiny parts â†’ merge storms, CPU waste
Too large parts â†’ slow merges, memory spikes

Our goal is **fewer, well-sized parts**.

---

# 3ï¸âƒ£ Migration Playbook â€” Fixing Bad Schemas Safely

Never mutate broken schemas in-place.

### ğŸŸ¢ Correct Approach: Shadow Table Migration

1. Create a **new table** with:

   * Correct partitioning
   * Correct codecs
   * Correct ordering keys
2. Backfill with **controlled batch copy**
3. Dual-write if required
4. Validate row counts + query correctness
5. Switch consumers
6. Decommission old table

---

### âœ¨ Example: Old â†’ New Table

```sql
CREATE TABLE new_table (...) 
ENGINE = MergeTree
PARTITION BY toYYYYMM(date)
ORDER BY (postcode1, postcode2, addr1, addr2);
```

Migration copy pattern

```sql
INSERT INTO new_table
SELECT * FROM old_table
WHERE date >= '...'
```

Batch in **time slices** to avoid merge pressure.

---

# 4ï¸âƒ£ Ingestion Hardening for Bursty Workloads

### âœ… Enable controlled async inserts

```sql
SET async_insert = 1
SET wait_for_async_insert = 0
```

App guidance:

* Batch **100Kâ€“500K rows**
* Use **2â€“4 parallel workers**
* Add **retry with exponential backoff**

---

### ğŸ§¯ Backpressure Protection (Required)

```xml
parts_to_delay_insert = 200
parts_to_throw_insert = 400
min_rows_for_wide_part = 100000
min_bytes_for_wide_part = 10MB
background_pool_size = 16â€“20
```

Platform rule:

> Inserts must slow down before the cluster melts down.

---

# 5ï¸âƒ£ Codec Strategy â€” Compression With Intent

We optimize for:

* Less IO
* Lower cost
* Faster scans on read workloads

Codec cheat sheet (operationally safe defaults) is preserved from prior version.

---

# 6ï¸âƒ£ Observability & SLO Signals (Platform Requirement)

A ClickHouse cluster is **healthy** when:

* Parts pending merge = stable
* Parts per partition remain below threshold
* Background pool queue does not grow
* Insert latency does not trend upward
* Query latency does not degrade under load

### ğŸš¨ Alert Before Failure

* Merge queue > threshold
* Parts per partition trending upward
* Async insert backlog aging
* Disk growth exponential
* Queries begin scanning > expected partitions

---

# 7ï¸âƒ£ Pre-Onboarding Checklist for New Services

Teams must provide:

* Expected ingest rate (avg + peak)
* Query date ranges
* Retention policy
* Backfill expectations
* Read-after-write requirements
* Burst tolerance characteristics
* Failure-mode expectations

Unsafe defaults are **blocked at design review**.

---

## ğŸ¯ Final Principle

This platform is designed around reliability discipline:

* Fewer knobs doing predictable work
* Stable merges instead of chaotic merges
* Explicit ingestion trade-offs
* No tables that surprise operators later

ClickHouse performs extremely well **when the storage layout matches workload reality** â€” and fails loudly when it doesnâ€™t.

Design intentionally.
