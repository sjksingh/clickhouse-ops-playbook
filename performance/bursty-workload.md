# ğŸ“Š ClickHouse Ingestion & Partitioning Guide â€” Lessons from Real Production Workloads

This document explains how partitioning, part sizing, batching, compression, and merge settings impact performance in ClickHouse â€” especially under **continuous ingestion and heavy-read analytics workloads**.

Itâ€™s written from a **production DBRE perspective**: the goal is not perfection, but **predictability, guardrails, and minimizing blast radius when ingestion or merges fall behind**.

---

## âœ… What This Guide Helps You Avoid

* Small part explosion
* Endless merge backlogs
* Hot partitions
* Slow queries from too many files
* Write amplification & CPU waste
* Insert throttling & ingestion stalls

---

# 1ï¸âƒ£ Partitioning Strategy â€” Where Things Commonly Go Wrong

### âŒ What went wrong

```sql
PARTITION BY toYear(date)
```

This creates **one partition per YEAR**.

**Problems:**

* All rows for a year land in a single partition
* Large partitions = **slow merges + huge parts**
* Inserts compete inside the same merge queue
* Retention deletes canâ€™t be granular
* Operational blast radius increases

ClickHouse merges happen **per partition**, so oversized yearly partitions lead to **long merge cycles and stuck small parts**.

---

### ğŸ§  Questions you should ask before choosing a partition key

| Question                         | Why It Matters                           |
| -------------------------------- | ---------------------------------------- |
| How far back do we query?        | Drives hot vs cold storage decisions     |
| Do queries span multiple years?  | Cross-partition scans impact latency     |
| What is the retention policy?    | Partition boundaries control DROP cost   |
| What is the typical query range? | Daily? Weekly? Monthly?                  |
| Will we delete/archive old data? | `DROP PARTITION` is instant â€” if aligned |

---

# 2ï¸âƒ£ Part Size Sweet Spot â€” The Science Behind It

### ğŸ¯ Recommended Part Size (ClickHouse best-practice range)

| Metric    | Minimum | Optimal       | Maximum |
| --------- | ------- | ------------- | ------- |
| **Rows**  | 100K    | 1â€“10M         | 100M    |
| **Bytes** | 10 MB   | 100 MB â€“ 1 GB | 10 GB   |

### Why this matters

#### Too small (< 100K rows)

* Many files â†’ slower queries
* Merge CPU waste
* Extra ZooKeeper chatter (replicated tables)

#### Too large (> 100M rows)

* Merges take hours
* Memory spikes
* Harder to parallelize reads

---

### ğŸ“‰ Your ingestion profile example

* 500K rows every 5 minutes
* If each insert becomes a part:

```
12 parts per hour
288 parts per day
500K rows per part
```

Borderline acceptable â€” but grows unstable under bursts or replication.

---

### ğŸ§  Questions to ask Engineering

| Question                               | Why                               |
| -------------------------------------- | --------------------------------- |
| What is sustained insert rate?         | Determines merge headroom         |
| Is ingestion bursty?                   | Bursts = small part avalanche     |
| How many tables ingest simultaneously? | Merge pool contention             |
| What is peak load?                     | Tune for worst-case, not averages |

---

# 3ï¸âƒ£ Choosing the Right Partition Granularity â€” Decision Framework

```
Partition Strategy Decision Tree
```

**< 100M rows/day**

* Queries = last 30 days â†’ `toYYYYMM()` âœ… BEST
* Queries = last 7 days â†’ weekly partitions âœ… OK
* Real-time analytics â†’ daily partitions âš ï¸ use carefully

**> 100M rows/day (high-volume)**

* Queries = last 24h â†’ daily partitions âœ…
* Short-range scans â†’ hourly partitions âš ï¸ advanced
* Extreme streaming â†’ `tuple()` (no partition) ğŸ”´ expert-only

---

### ğŸ“Œ Example Case

```
500K rows / 5 min  â†’ 144M rows/year
Future: 5M / 5 min â†’ 1.44B rows/year
```

| Strategy | Partitions/Year | Rows/Partition (current) | Rows/Partition (future) | Verdict            |
| -------- | --------------: | -----------------------: | ----------------------: | ------------------ |
| YEAR     |               1 |                     144M |                   1.44B | ğŸ”´ Too coarse      |
| MONTH    |              12 |                      12M |                    120M | âœ… Balanced         |
| WEEK     |              52 |                     2.7M |                     27M | âš ï¸ Higher overhead |
| DAY      |             365 |                     400K |                      4M | ğŸ”´ Too many parts  |

ğŸ‘‰ **Recommendation: `PARTITION BY toYYYYMM(date)` (Monthly)**

---

### ğŸ§  Questions for PM / Platform

| Question                       | Why                       |
| ------------------------------ | ------------------------- |
| Typical query window?          | Guides partition horizon  |
| Do we archive / drop old data? | Monthly drops = instant   |
| Expected growth x10 / x100?    | Future-proofing           |
| Retention / compliance rules?  | Impacts partition roll-up |

---

# 4ï¸âƒ£ Go Client Batching â€” Sync vs Async Inserts

### âŒ Typical synchronous batching

```go
batch, _ := conn.PrepareBatch(ctx, "INSERT INTO uk_price_paid")
for _, row := range rows {
    batch.Append(...)
}
batch.Send() // Blocks
```

Simple â€” but blocks the app and prevents pipelining.

---

### âœ… Recommended: Async Inserts

```go
batch, _ := conn.PrepareBatch(
    ctx,
    "INSERT INTO uk_price_paid SETTINGS async_insert=1, wait_for_async_insert=0",
)
```

**Benefits**

* ClickHouse buffers & merges inserts
* Larger parts â†’ fewer merges
* App threads do not stall

**Trade-off**

* Data becomes visible with ~1â€“2s delay

**Good defaults for high-volume ingestion**

* Batch size: **100Kâ€“500K rows**
* Parallel workers: **2â€“4 goroutines**

---

### ğŸ§  Questions to ask Engineering

| Question                         | Why                     |
| -------------------------------- | ----------------------- |
| Can we tolerate 1â€“2s ingest lag? | Async viability         |
| Is read-after-write required?    | If yes â†’ sync or hybrid |
| Concurrency model?               | Worker tuning           |
| Retry strategy?                  | Avoid silent data loss  |

---

# 5ï¸âƒ£ Compression Codecs â€” Where Performance Is Won or Lost

### ğŸ¯ Optimized Table Example

```sql
CREATE TABLE uk_price_paid_optimized (
    price UInt32 CODEC(DoubleDelta, ZSTD(3)),
    date Date CODEC(DoubleDelta, LZ4),

    postcode1 LowCardinality(String),
    postcode2 LowCardinality(String),

    type Enum8(...),
    is_new UInt8 CODEC(T64, ZSTD),

    duration Enum8(...),

    addr1 String CODEC(ZSTD(3)),
    addr2 String CODEC(ZSTD(3)),
    street LowCardinality(String),
    locality LowCardinality(String),
    town LowCardinality(String),
    district LowCardinality(String),
    county LowCardinality(String)
)
ENGINE = MergeTree
PARTITION BY toYYYYMM(date)
ORDER BY (postcode1, postcode2, addr1, addr2)
SETTINGS index_granularity = 8192;
```

---

### ğŸ§© Codec Selection Cheat-Sheet

| Pattern                 | Best Codec         | Example    |
| ----------------------- | ------------------ | ---------- |
| Sequential numbers      | DoubleDelta + LZ4  | timestamps |
| Slowly changing numeric | DoubleDelta + ZSTD | price      |
| Random numeric          | LZ4 / ZSTD(1)      | ids        |
| Boolean flags           | T64 + ZSTD         | is_active  |
| Low-card strings        | LowCardinality     | city       |
| Very low-card           | Enum8 / Enum16     | type       |
| Long text               | ZSTD(3â€“5)          | address    |

**Impact:** Smaller data â†’ fewer reads â†’ faster queries.

---

# 6ï¸âƒ£ Merge Settings â€” Preventing Small-Part Backlogs

### ğŸ”§ Key knobs

```xml
<merge_tree>
  <parts_to_throw_insert>400</parts_to_throw_insert>
  <parts_to_delay_insert>200</parts_to_delay_insert>

  <min_rows_for_wide_part>100000</min_rows_for_wide_part>
  <min_bytes_for_wide_part>10485760</min_bytes_for_wide_part> <!-- 10 MB -->
</merge_tree>

<background_pool_size>20</background_pool_size>
```

### Suggested defaults for ingestion workloads

| Setting                       | Why                                |
| ----------------------------- | ---------------------------------- |
| `parts_to_delay_insert=200`   | Apply backpressure before meltdown |
| `parts_to_throw_insert=400`   | Hard stop to protect cluster       |
| `min_rows_for_wide_part=100K` | Avoid tiny parts                   |
| `background_pool_size=16â€“20`  | More merge capacity                |

---

### ğŸ§  Questions to ask Engineering

| Question                             | Why                           |
| ------------------------------------ | ----------------------------- |
| What happens when inserts fail?      | Backpressure tolerance        |
| Can ingestion pause for maintenance? | Merge scheduling              |
| Acceptable insert latency?           | Delay vs throughput trade-off |

---

# ğŸ§¾ Deployment Readiness Checklist (For Every New Table)

### ğŸ“Œ Data Characteristics

* Sustained insert rate (rows/sec)?
* Peak ingestion?
* Growth horizon (6â€“36 months)?
* Column cardinality?
* Sequential or random distribution?

### ğŸ” Query Patterns

* Typical query window?
* Most common filter columns?
* Group-by fields?
* Latency target?
* Point lookups vs aggregations?

### ğŸ—‚ Retention & Lifecycle

* Retention policy?
* Archive vs delete?
* Compliance rules?
* PITR requirements?

### ğŸšš Ingestion Architecture

* Client language / driver?
* Async inserts allowed?
* Read-after-write required?
* Retry guarantees?
* Burst vs steady ingestion?

### ğŸ›  Operational Constraints

* Disk budget?
* CPU budget?
* Can ingestion pause?
* Monitoring & alerting coverage?

---

## ğŸ¯ Final Takeaway

We donâ€™t optimize for theoretical perfection.
We optimize for **operational stability, predictable merges, and bounded failure modes** â€” especially under heavy ingestion.

Design your **partitioning, batching, compression, and merge strategy intentionally** â€” not accidentally.

---

