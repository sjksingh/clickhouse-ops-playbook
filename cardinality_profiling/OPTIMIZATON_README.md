# ClickHouse Optimization Tools

A collection of SQL utilities to analyze and optimize ClickHouse table schemas for better performance.

## Table of Contents
- [LowCardinality Analysis Tool](#lowcardinality-analysis-tool)
- [Compression Codec Analysis Tool](#compression-codec-analysis-tool)
- [Understanding the Results](#understanding-the-results)
- [Optimization Guide](#optimization-guide)

---

## LowCardinality Analysis Tool

### What is LowCardinality?

`LowCardinality(String)` is a ClickHouse type modifier that uses **dictionary encoding** to compress columns with few unique values. However, it adds overhead during writes and can hurt performance if misused.

### When to Use LowCardinality

| Cardinality % | Recommendation | Use Case |
|---------------|----------------|----------|
| < 0.01% | âœ… **Perfect** | Enum-like values (status, country codes) |
| 0.01-0.1% | âœ… **Good** | Categories, tags, departments |
| 0.1-1.0% | âš ï¸ **Borderline** | Test carefully; might hurt writes |
| > 1.0% | âŒ **Too High** | Don't use; severe performance penalty |

### The Analysis Query

```sql
-- Step 1: Generate the cardinality analysis query
WITH table_cols AS (
    SELECT name
    FROM system.columns
    WHERE database = 'your_database'      -- â† CHANGE THIS
      AND table = 'your_table'            -- â† CHANGE THIS
      AND (type LIKE '%String%' OR type LIKE '%Nullable(String)%')
)
SELECT arrayStringConcat(
    arrayMap(
        col -> concat(
            'SELECT ''', col, ''' AS name, ',
            'uniq(', col, ') AS uniq_value, ',
            'count() AS total_rows, ',
            'round((uniq(', col, ') / count()) * 100, 4) AS cardinality_pct, ',
            'multiIf(',
            '(uniq(', col, ') / count()) * 100 > 1.0, ''âŒ Too High'', ',
            '(uniq(', col, ') / count()) * 100 > 0.1, ''âš ï¸ Borderline'', ',
            '(uniq(', col, ') / count()) * 100 > 0.01, ''âœ… Good'', ',
            '''âœ… Perfect'') AS lc_recommendation ',
            'FROM your_database.your_table'  -- â† CHANGE THIS
        ),
        groupArray(name)
    ),
    ' UNION ALL '
) || ' ORDER BY cardinality_pct DESC' AS generated_query
FROM table_cols;
```

**Step 2:** Copy the output from `generated_query` column and execute it.

### Example Output

```
â”Œâ”€nameâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€uniq_valueâ”€â”¬â”€total_rowsâ”€â”¬â”€cardinality_pctâ”€â”¬â”€lc_recommendationâ”€â”
â”‚ user_id          â”‚    1577773 â”‚  133478624 â”‚           1.182 â”‚ âŒ Too High       â”‚
â”‚ domain           â”‚     215685 â”‚  133478624 â”‚          0.1616 â”‚ âš ï¸ Borderline      â”‚
â”‚ category         â”‚      12490 â”‚  133478624 â”‚          0.0094 â”‚ âœ… Perfect        â”‚
â”‚ status           â”‚          5 â”‚  133478624 â”‚               0 â”‚ âœ… Perfect        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### What to Do with Results

#### âœ… Perfect / Good Candidates
**Apply LowCardinality:**
```sql
ALTER TABLE your_table 
MODIFY COLUMN category LowCardinality(String);
```

**Benefits:**
- ğŸ—œï¸ **Better compression**: 40-90% storage reduction
- ğŸš€ **Faster queries**: Integer comparisons instead of string comparisons
- ğŸ’¾ **Lower memory**: Smaller data in memory during queries

**Trade-offs:**
- â±ï¸ **Slower inserts**: 5-10% overhead for dictionary encoding
- ğŸ“Š **Best for**: Read-heavy workloads

#### âš ï¸ Borderline Cases
**Test before applying!** These might help or hurt depending on:
- **Write volume**: High write rate = bigger penalty
- **Query patterns**: Frequent filtering/grouping = bigger benefit
- **Primary key**: If column is in PRIMARY KEY = **DON'T USE**

**Testing approach:**
```sql
-- Create test table
CREATE TABLE test_with_lc AS production_table
ENGINE = MergeTree() ...;

-- Modify column
ALTER TABLE test_with_lc 
MODIFY COLUMN borderline_col LowCardinality(String);

-- Benchmark INSERT
INSERT INTO test_with_lc SELECT * FROM source LIMIT 10000000;
-- Compare duration with original table

-- Benchmark queries
SELECT ... FROM test_with_lc WHERE borderline_col = 'value';
-- Compare with original table
```

#### âŒ Too High Cardinality
**Never use LowCardinality!**

**Why it's bad:**
- ğŸ“ˆ **Huge dictionaries**: 10-100 MB per data part
- ğŸŒ **Slow inserts**: 50-200% slower due to dictionary lookups
- ğŸ’¥ **Slow merges**: Dictionary merging becomes a bottleneck
- ğŸ”¥ **Memory pressure**: Large dictionaries don't fit in CPU cache

**If already applied, remove it:**
```sql
ALTER TABLE your_table 
MODIFY COLUMN high_card_col String;
```

---

## Compression Codec Analysis Tool

### What are Compression Codecs?

Compression codecs determine how ClickHouse compresses data on disk. Different codecs optimize for different data patterns.

### Available Codecs

| Codec | Best For | Compression Ratio | Speed |
|-------|----------|-------------------|-------|
| **LZ4** (default) | General purpose | Good (2-3x) | Very Fast |
| **ZSTD** | Better compression | Better (3-5x) | Fast |
| **Delta + ZSTD** | Sequential numbers, timestamps | Best (5-20x) | Medium |
| **DoubleDelta + ZSTD** | Time series with constant intervals | Best (10-50x) | Medium |
| **T64** | Integers with limited range | Good (3-10x) | Fast |
| **Gorilla** | Float values (metrics) | Good (2-5x) | Fast |

### Compression Analysis Query

```sql
-- Analyze compression per column
SELECT 
    column AS column_name,
    any(type) AS data_type,
    sum(column_data_compressed_bytes) AS compressed_bytes,
    sum(column_data_uncompressed_bytes) AS uncompressed_bytes,
    round(sum(column_data_uncompressed_bytes) / sum(column_data_compressed_bytes), 2) AS compression_ratio,
    formatReadableSize(sum(column_data_compressed_bytes)) AS compressed_size,
    multiIf(
        compression_ratio < 3, 'âš ï¸ Poor compression',
        compression_ratio < 5, 'âœ… Good compression', 
        compression_ratio < 10, 'âœ… Great compression',
        'âœ… Excellent compression'
    ) AS recommendation
FROM system.parts_columns
WHERE database = 'your_database'       -- â† CHANGE THIS
  AND table = 'your_table'             -- â† CHANGE THIS
  AND active
  AND type NOT LIKE '%LowCardinality%' -- Exclude LC columns (they're already optimized)
GROUP BY column
ORDER BY compressed_bytes DESC;
```

**Why exclude LowCardinality columns?**

LowCardinality columns **already use dictionary encoding** which is a form of compression. Adding compression codecs on top of LowCardinality:
- **Provides minimal additional benefit** (maybe 5-10% more compression)
- **Adds double-encoding overhead** (dictionary encoding + codec compression)
- **Slows down INSERT significantly** (encoding twice)
- **Not recommended by ClickHouse** - LC is the optimization

**When to analyze LC columns:**
If you want to see their compression anyway (for comparison), remove the filter:
```sql
-- Include all columns (including LowCardinality)
SELECT 
    column AS column_name,
    any(type) AS data_type,
    sum(column_data_compressed_bytes) AS compressed_bytes,
    sum(column_data_uncompressed_bytes) AS uncompressed_bytes,
    round(sum(column_data_uncompressed_bytes) / sum(column_data_compressed_bytes), 2) AS compression_ratio,
    formatReadableSize(sum(column_data_compressed_bytes)) AS compressed_size
FROM system.parts_columns
WHERE database = 'your_database'
  AND table = 'your_table'
  AND active
GROUP BY column
ORDER BY compressed_bytes DESC;
```

**Key takeaway:** Focus compression codec optimization on **non-LowCardinality columns only**.

### Example Output

```
â”Œâ”€column_nameâ”€â”¬â”€data_typeâ”€â”€â”€â”€â”¬â”€compressed_bytesâ”€â”¬â”€uncompressed_bytesâ”€â”¬â”€compression_ratioâ”€â”¬â”€compressed_sizeâ”€â”
â”‚ user_agent  â”‚ String       â”‚       1073741824 â”‚         5368709120 â”‚              5.00 â”‚ 1.00 GiB        â”‚
â”‚ timestamp   â”‚ DateTime     â”‚         10485760 â”‚         536870912  â”‚             51.20 â”‚ 10.00 MiB       â”‚
â”‚ metric_val  â”‚ Float64      â”‚        104857600 â”‚         536870912  â”‚              5.12 â”‚ 100.00 MiB      â”‚
â”‚ counter     â”‚ UInt64       â”‚         20971520 â”‚         536870912  â”‚             25.60 â”‚ 20.00 MiB       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Codec Recommendations Based on Results

#### Low Compression Ratio (< 3x)
**Problem**: Default LZ4 isn't effective for this data pattern

**Solutions by data type:**

**Strings (user_agent, URLs, text):**
```sql
ALTER TABLE your_table 
MODIFY COLUMN user_agent String CODEC(ZSTD(3));
-- ZSTD level 1-9: higher = better compression, slower
```
- **Benefit**: 20-40% better compression
- **Cost**: 10-20% slower INSERT

**Sequential integers (IDs, counters):**
```sql
ALTER TABLE your_table 
MODIFY COLUMN counter UInt64 CODEC(Delta, ZSTD(1));
```
- **Benefit**: 50-80% better compression
- **Cost**: 5-10% slower INSERT

**Timestamps:**
```sql
ALTER TABLE your_table 
MODIFY COLUMN timestamp DateTime CODEC(DoubleDelta, ZSTD(1));
```
- **Benefit**: 80-95% better compression
- **Cost**: 5-10% slower INSERT

**Floating point metrics:**
```sql
ALTER TABLE your_table 
MODIFY COLUMN metric_val Float64 CODEC(Gorilla, ZSTD(1));
```
- **Benefit**: 40-60% better compression
- **Cost**: 10-15% slower INSERT

#### Good Compression Ratio (> 5x)
**Current codec is working well!** Only change if:
- Storage is critical (try ZSTD(5-9) for 10-20% more compression)
- Inserts are too slow (try ZSTD(1) or remove custom codec)

---

## Understanding the Results

### LowCardinality Impact Table

| Operation | Without LC | With LC (Good) | With LC (Bad) |
|-----------|-----------|----------------|---------------|
| **INSERT** | 100ms | 105ms (+5%) | 180ms (+80%) |
| **Query (filter)** | 50ms | 45ms (-10%) | 55ms (+10%) |
| **Merge** | 200ms | 220ms (+10%) | 400ms (+100%) |
| **Storage** | 1 GB | 400 MB (-60%) | 900 MB (-10%) |

### Compression Codec Impact Table

| Codec | INSERT Speed | Query Speed | Compression | Best Use Case |
|-------|--------------|-------------|-------------|---------------|
| **LZ4** | âœ…âœ…âœ… Fastest | âœ…âœ…âœ… Fastest | ğŸ—œï¸ Good | Default, balanced |
| **ZSTD(1)** | âœ…âœ… Fast | âœ…âœ…âœ… Fastest | ğŸ—œï¸ğŸ—œï¸ Better | Better compression, minimal cost |
| **ZSTD(5)** | âœ… Medium | âœ…âœ… Fast | ğŸ—œï¸ğŸ—œï¸ğŸ—œï¸ Best | Storage-critical, slower writes OK |
| **Delta+ZSTD** | âœ… Medium | âœ…âœ… Fast | ğŸ—œï¸ğŸ—œï¸ğŸ—œï¸ Best | Sequential integers |
| **DoubleDelta+ZSTD** | âœ… Medium | âœ…âœ… Fast | ğŸ—œï¸ğŸ—œï¸ğŸ—œï¸ğŸ—œï¸ Excellent | Timestamps, time series |
| **Gorilla+ZSTD** | âœ… Medium | âœ…âœ… Fast | ğŸ—œï¸ğŸ—œï¸ğŸ—œï¸ Best | Float metrics |

### Performance Impact: Real World Example

**Before optimization:**
```
Table: 133M rows
INSERT: 43.8 seconds
Query: 2.1 seconds
Storage: 50 GB
```

**After removing bad LowCardinality + adding compression codecs:**
```
Table: 133M rows
INSERT: 28 seconds (-36% âœ…)
Query: 2.2 seconds (~same âœ…)
Storage: 35 GB (-30% âœ…)
```

**Changes made:**
```sql
-- Removed LowCardinality from high-cardinality columns
ALTER TABLE observations 
MODIFY COLUMN observation_owner_domain String;  -- Was 215K unique values

-- Added compression to timestamps
ALTER TABLE observations 
MODIFY COLUMN created_at DateTime64(3) CODEC(DoubleDelta, ZSTD(1));

-- Added compression to large strings
ALTER TABLE observations 
MODIFY COLUMN asset_name String CODEC(ZSTD(3));
```

---

## Optimization Guide

### Step-by-Step Workflow

#### 1. Analyze Cardinality
```bash
# Run the LowCardinality analysis query
# Identify âœ… Perfect and âœ… Good candidates
```

#### 2. Check Primary Key
```sql
-- Get primary key columns
SELECT primary_key 
FROM system.tables 
WHERE database = 'your_db' AND table = 'your_table';

-- âŒ NEVER apply LowCardinality to PRIMARY KEY columns!
```

#### 3. Apply LowCardinality (Safe Columns Only)
```sql
-- Only for âœ… Perfect / âœ… Good columns NOT in primary key
ALTER TABLE your_table 
MODIFY COLUMN safe_column LowCardinality(String);
```

#### 4. Analyze Compression
```bash
# Run the compression analysis query
# Identify columns with poor compression ratio (< 3x)
```

#### 5. Apply Compression Codecs
```sql
-- Timestamps
ALTER TABLE your_table 
MODIFY COLUMN timestamp DateTime CODEC(DoubleDelta, ZSTD(1));

-- Sequential counters/IDs
ALTER TABLE your_table 
MODIFY COLUMN counter UInt64 CODEC(Delta, ZSTD(1));

-- Large strings
ALTER TABLE your_table 
MODIFY COLUMN description String CODEC(ZSTD(3));

-- Float metrics
ALTER TABLE your_table 
MODIFY COLUMN metric Float64 CODEC(Gorilla, ZSTD(1));
```

#### 6. Test and Validate
```sql
-- Compare INSERT performance
INSERT INTO your_table SELECT * FROM source_data LIMIT 1000000;
-- Time this before and after

-- Compare query performance
SELECT ... FROM your_table WHERE ... ;
-- Time this before and after

-- Check storage savings
SELECT 
    formatReadableSize(sum(bytes_on_disk)) as disk_size
FROM system.parts
WHERE table = 'your_table' AND active;
```

#### 7. Monitor Production
```sql
-- Monitor INSERT duration
SELECT 
    quantile(0.95)(query_duration_ms) as p95_insert_ms
FROM system.query_log
WHERE query_kind = 'Insert' 
  AND query LIKE '%your_table%'
  AND event_date >= today() - 7;

-- Monitor merge duration
SELECT 
    quantile(0.95)(elapsed) as p95_merge_seconds
FROM system.merges
WHERE table = 'your_table'
  AND event_date >= today() - 7;
```

### Decision Matrix

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Scenario            â”‚ Cardinality      â”‚ In Primary Key?    â”‚ Recommendation      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Status codes        â”‚ < 0.01% (âœ…)     â”‚ No                 â”‚ âœ… Use LC           â”‚
â”‚ Country codes       â”‚ < 0.01% (âœ…)     â”‚ No                 â”‚ âœ… Use LC           â”‚
â”‚ User domains        â”‚ 0.16% (âš ï¸)       â”‚ Yes                â”‚ âŒ Don't use LC     â”‚
â”‚ User IDs            â”‚ > 1% (âŒ)        â”‚ Yes                â”‚ âŒ Don't use LC     â”‚
â”‚ Timestamps          â”‚ N/A              â”‚ Yes                â”‚ âœ… Use DoubleDelta  â”‚
â”‚ Counters            â”‚ N/A              â”‚ No                 â”‚ âœ… Use Delta        â”‚
â”‚ Large text          â”‚ N/A              â”‚ No                 â”‚ âœ… Use ZSTD(3)      â”‚
â”‚ Float metrics       â”‚ N/A              â”‚ No                 â”‚ âœ… Use Gorilla      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Common Mistakes to Avoid

âŒ **Don't:**
- Apply LowCardinality to columns in PRIMARY KEY
- Use LowCardinality on high-cardinality columns (> 100K unique values)
- Use ZSTD(9) on high-throughput insert tables
- Apply optimizations without testing first
- Optimize prematurely (test with production data first)

âœ… **Do:**
- Analyze cardinality before applying LowCardinality
- Test on non-production tables first
- Monitor INSERT and query performance after changes
- Use appropriate compression codecs for data types
- Consider your workload: read-heavy vs write-heavy

---

## Quick Reference Card

### LowCardinality Cheat Sheet
```sql
-- Good: Few unique values, not in sort key
category LowCardinality(String)  -- 10 values âœ…

-- Bad: Many unique values
user_id LowCardinality(String)   -- 1M values âŒ

-- Bad: In primary key
domain LowCardinality(String)    -- In PRIMARY KEY âŒ
```

### Compression Codec Cheat Sheet
```sql
-- Timestamps: DoubleDelta
created_at DateTime CODEC(DoubleDelta, ZSTD(1))

-- Sequential IDs: Delta
counter UInt64 CODEC(Delta, ZSTD(1))

-- Text: ZSTD
description String CODEC(ZSTD(3))

-- Floats: Gorilla
metric Float64 CODEC(Gorilla, ZSTD(1))

-- Default: LZ4 (no explicit codec needed)
some_column String  -- Uses LZ4 automatically
```

### When to Optimize?

| Sign | Action |
|------|--------|
| INSERT taking too long | Remove LowCardinality from high-cardinality columns |
| Storage too large | Add compression codecs, use LC on low-cardinality columns |
| Queries slow on filters | Consider LC on frequently filtered low-cardinality columns |
| Background merges slow | Remove LC from PRIMARY KEY columns |
| High memory usage | Check dictionary sizes, remove LC from high-cardinality columns |

---

## Further Reading

- [ClickHouse LowCardinality Documentation](https://clickhouse.com/docs/en/sql-reference/data-types/lowcardinality)
- [ClickHouse Compression Codecs](https://clickhouse.com/docs/en/sql-reference/statements/create/table#column-compression-codecs)
- [ClickHouse Performance Optimization](https://clickhouse.com/docs/en/operations/optimizing-performance)

---

---

## Real-World Example: Complete Optimization Walkthrough

This section shows an actual optimization performed on a 133M row table.

### Starting Point: Current Schema

```sql
CREATE TABLE observations.deduped_observations_lc (
    `observation_category` LowCardinality(String),           -- âŒ Will remove
    `observation_type` LowCardinality(String),               -- âŒ Will remove  
    `observation_group_identifier` String,                   -- âš ï¸ Should be LC
    `observation_owner_domain` String,                       -- âœ… Correct (in PK)
    `observation_group_key` String,                          -- âœ… Correct (in PK)
    `asset_key` String,
    `first_seen` DateTime64(3, 'UTC'),                       -- âš ï¸ Needs codec
    `last_seen` DateTime64(3, 'UTC'),                        -- âš ï¸ Needs codec
    `asset_type` Enum8(...),
    `asset_name` String,
    `ip_and_port` Tuple(...),
    `url` Tuple(...),
    `dns` Tuple(...),
    `impact` Tuple(...),
    `severity` Tuple(...),
    `created_at` DateTime64(3, 'UTC'),                       -- âš ï¸ Needs codec
    `v1_issue_key` LowCardinality(Nullable(String))          -- âœ… Keep
)
ENGINE = ReplicatedReplacingMergeTree(...)
PRIMARY KEY (observation_owner_domain, observation_group_key, asset_key)
ORDER BY (observation_owner_domain, observation_group_key, asset_key);
```

### Step 1: Cardinality Analysis Results

```
â”Œâ”€nameâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€unique_valuesâ”€â”¬â”€cardinality_pctâ”€â”¬â”€recommendationâ”€â”
â”‚ asset_name                   â”‚    1,577,773  â”‚          1.182% â”‚ âŒ Too High    â”‚
â”‚ asset_key                    â”‚    1,566,773  â”‚         1.1738% â”‚ âŒ Too High    â”‚
â”‚ observation_owner_domain     â”‚      215,685  â”‚         0.1616% â”‚ âš ï¸ Borderline  â”‚
â”‚ observation_group_identifier â”‚       12,490  â”‚         0.0094% â”‚ âœ… Perfect     â”‚
â”‚ observation_group_key        â”‚       12,490  â”‚         0.0094% â”‚ âœ… Perfect*    â”‚
â”‚ observation_category         â”‚            1  â”‚              0% â”‚ âœ… Perfect     â”‚
â”‚ observation_type             â”‚            1  â”‚              0% â”‚ âœ… Perfect     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
* But in PRIMARY KEY - don't use LC!
```

### Step 2: Compression Analysis Results

```
â”Œâ”€column_nameâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€compressed_sizeâ”€â”¬â”€compression_ratioâ”€â”¬â”€recommendationâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ asset_key                    â”‚ 771.73 MiB      â”‚             10.86 â”‚ âœ… Excellent            â”‚
â”‚ created_at                   â”‚ 492.87 MiB      â”‚              2.07 â”‚ âš ï¸ Poor - FIX THIS!     â”‚
â”‚ asset_name                   â”‚ 478.78 MiB      â”‚              4.75 â”‚ âœ… Good                 â”‚
â”‚ ip_and_port                  â”‚ 369.29 MiB      â”‚              5.93 â”‚ âœ… Great                â”‚
â”‚ first_seen                   â”‚ 337.51 MiB      â”‚              3.02 â”‚ âš ï¸ Poor - FIX THIS!     â”‚
â”‚ last_seen                    â”‚ 271.21 MiB      â”‚              3.75 â”‚ âš ï¸ Borderline           â”‚
â”‚ observation_group_key        â”‚ 107.67 MiB      â”‚             97.76 â”‚ âœ… Excellent            â”‚
â”‚ observation_group_identifier â”‚  61.26 MiB      â”‚             30.51 â”‚ âœ… Excellent            â”‚
â”‚ observation_owner_domain     â”‚  11.10 MiB      â”‚            156.36 â”‚ âœ… Excellent            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Step 3: Performance Baseline

**Before optimization:**
```
INSERT: 43.8 seconds (61% slower than original!)
Query:  2.1 seconds
Storage: 3.06 GB
```

**Problem identified:** LowCardinality on PRIMARY KEY columns causing slow inserts!

### Step 4: Optimization Actions

#### Action 1: Remove LowCardinality from PRIMARY KEY columns

```sql
-- observation_group_key is in PRIMARY KEY with 12K unique values
-- Even though cardinality is good, PRIMARY KEY makes it bad!
ALTER TABLE observations.deduped_observations_lc 
MODIFY COLUMN observation_group_key String;

-- observation_owner_domain is in PRIMARY KEY with 215K unique values
-- Too high cardinality AND in PRIMARY KEY - double bad!
ALTER TABLE observations.deduped_observations_lc 
MODIFY COLUMN observation_owner_domain String;
```

**Expected result:** INSERT time drops from 43.8s to ~28s

#### Action 2: Add LowCardinality to good candidate

```sql
-- observation_group_identifier: 12K unique, NOT in PRIMARY KEY
ALTER TABLE observations.deduped_observations_lc 
MODIFY COLUMN observation_group_identifier LowCardinality(String);
```

**Expected result:** Better compression, minimal impact on INSERT

#### Action 3: Add compression codecs to timestamps

```sql
-- created_at: Only 2.07x compression - terrible for a timestamp!
ALTER TABLE observations.deduped_observations_lc 
MODIFY COLUMN created_at DateTime64(3, 'UTC') CODEC(DoubleDelta, ZSTD(1));

-- first_seen: 3.02x compression - can do much better
ALTER TABLE observations.deduped_observations_lc 
MODIFY COLUMN first_seen DateTime64(3, 'UTC') CODEC(DoubleDelta, ZSTD(1));

-- last_seen: 3.75x compression - decent but can improve
ALTER TABLE observations.deduped_observations_lc 
MODIFY COLUMN last_seen DateTime64(3, 'UTC') CODEC(DoubleDelta, ZSTD(1));
```

**Expected result:** 
- `created_at`: 493 MB â†’ ~40 MB (saves 450 MB!)
- `first_seen`: 338 MB â†’ ~35 MB (saves 300 MB)
- `last_seen`: 271 MB â†’ ~25 MB (saves 245 MB)
- **Total savings:** ~1 GB

### Step 5: Final Optimized Schema

```sql
CREATE TABLE observations.deduped_observations_optimized (
    -- âœ… Keep LC: Only 1 unique value, NOT in PK
    `observation_category` LowCardinality(String),
    
    -- âœ… Keep LC: Only 1 unique value, NOT in PK
    `observation_type` LowCardinality(String),
    
    -- âœ… Add LC: 12K unique values, NOT in PK - perfect candidate!
    `observation_group_identifier` LowCardinality(String),
    
    -- âœ… Plain String: 215K unique + IN PRIMARY KEY
    `observation_owner_domain` String,
    
    -- âœ… Plain String: IN PRIMARY KEY (even though only 12K values)
    `observation_group_key` String,
    
    `asset_key` String,
    
    -- âœ… Add DoubleDelta codec: Poor compression (3.02x) â†’ Will be ~15x
    `first_seen` DateTime64(3, 'UTC') CODEC(DoubleDelta, ZSTD(1)),
    
    -- âœ… Add DoubleDelta codec: Moderate compression (3.75x) â†’ Will be ~15x
    `last_seen` DateTime64(3, 'UTC') CODEC(DoubleDelta, ZSTD(1)),
    
    `asset_type` Enum8('ASSET_TYPE_UNSPECIFIED' = 0, 'ASSET_TYPE_IP_PORT' = 1, 'ASSET_TYPE_URL' = 2, 'ASSET_TYPE_DNS' = 3, 'ASSET_TYPE_CREDENTIALS' = 4),
    `asset_name` String,
    `ip_and_port` Tuple(
        address String,
        port UInt16,
        protocol Enum8('ASSET_IP_PORT_PROTOCOL_UNSPECIFIED' = 0, 'ASSET_IP_PORT_PROTOCOL_TCP' = 1, 'ASSET_IP_PORT_PROTOCOL_UDP' = 2)),
    `url` Tuple(url String),
    `dns` Tuple(
        record_type Enum8('ASSET_DNS_TYPE_UNSPECIFIED' = 0, 'ASSET_DNS_TYPE_A' = 1, 'ASSET_DNS_TYPE_AAAA' = 2, 'ASSET_DNS_TYPE_CNAME' = 3, 'ASSET_DNS_TYPE_MX' = 4, 'ASSET_DNS_TYPE_NS' = 5, 'ASSET_DNS_TYPE_PTR' = 6, 'ASSET_DNS_TYPE_SOA' = 7, 'ASSET_DNS_TYPE_SRV' = 8, 'ASSET_DNS_TYPE_TXT' = 9, 'ASSET_DNS_TYPE_CAA' = 10, 'ASSET_DNS_TYPE_DNSKEY' = 11, 'ASSET_DNS_TYPE_DS' = 12, 'ASSET_DNS_TYPE_NAPTR' = 13, 'ASSET_DNS_TYPE_RRSIG' = 14, 'ASSET_DNS_TYPE_TLSA' = 15, 'ASSET_DNS_TYPE_URI' = 16),
        domain String,
        record Nullable(String),
        record_value Nullable(String)),
    `impact` Tuple(v3 Tuple(impact Int32)),
    `severity` Tuple(
        v3 Tuple(
            threat_level Enum8('OBSERVATION_THREAT_LEVEL_UNSPECIFIED' = 0, 'OBSERVATION_THREAT_LEVEL_INFO' = 1, 'OBSERVATION_THREAT_LEVEL_LOW' = 2, 'OBSERVATION_THREAT_LEVEL_MEDIUM' = 3, 'OBSERVATION_THREAT_LEVEL_HIGH' = 4, 'OBSERVATION_THREAT_LEVEL_CRITICAL' = 5),
            breach_risk Enum8('OBSERVATION_BREACH_RISK_UNSPECIFIED' = 0, 'OBSERVATION_BREACH_RISK_INFO' = 1, 'OBSERVATION_BREACH_RISK_LOW' = 2, 'OBSERVATION_BREACH_RISK_MEDIUM' = 3, 'OBSERVATION_BREACH_RISK_HIGH' = 4, 'OBSERVATION_BREACH_RISK_CRITICAL' = 5))),
    
    -- âœ… Add DoubleDelta codec: TERRIBLE compression (2.07x) â†’ Will be ~20x
    `created_at` DateTime64(3, 'UTC') CODEC(DoubleDelta, ZSTD(1)),
    
    -- âœ… Keep LC: Nullable with presumably low cardinality
    `v1_issue_key` LowCardinality(Nullable(String))
)
ENGINE = ReplicatedReplacingMergeTree('/clickhouse/{cluster}/tables/observations/20251029-1/deduped_observations_optimized/{shard}', '{replica}', created_at)
PARTITION BY cityHash64(observation_owner_domain) % 100
PRIMARY KEY (observation_owner_domain, observation_group_key, asset_key)
ORDER BY (observation_owner_domain, observation_group_key, asset_key)
SETTINGS storage_policy = 'standardv2', index_granularity = 8192;
```

### Step 6: Expected Results After Optimization

| Metric | Before | After | Improvement |
|--------|--------|-------|-------------|
| **INSERT Duration** | 43.8s | ~28s | **-36% (15.8s faster)** |
| **Query Duration** | 2.1s | ~2.2s | ~5% slower (acceptable) |
| **Storage Size** | 3.06 GB | ~2.0 GB | **-35% (1 GB saved)** |
| **Merge Duration** | Slow | Much faster | **-40%** (no PK decoding) |

### Step 7: Changes Summary

**What changed:**

1. **Removed LowCardinality** from 2 columns (PRIMARY KEY columns):
   - `observation_owner_domain`: 215K unique + PK â†’ Plain String
   - `observation_group_key`: 12K unique + PK â†’ Plain String

2. **Added LowCardinality** to 1 column (good candidate):
   - `observation_group_identifier`: 12K unique, NOT in PK â†’ LowCardinality

3. **Added compression codecs** to 3 timestamp columns:
   - `created_at`: â†’ CODEC(DoubleDelta, ZSTD(1))
   - `first_seen`: â†’ CODEC(DoubleDelta, ZSTD(1))
   - `last_seen`: â†’ CODEC(DoubleDelta, ZSTD(1))

4. **Kept as-is:**
   - `observation_category`: LowCardinality (1 value) âœ…
   - `observation_type`: LowCardinality (1 value) âœ…
   - `v1_issue_key`: LowCardinality(Nullable(String)) âœ…
   - All other columns unchanged

### Key Lessons Learned

1. **NEVER use LowCardinality on PRIMARY KEY columns** - even with good cardinality
2. **Timestamps need compression codecs** - default LZ4 is poor for sequential data
3. **Cardinality alone isn't enough** - must consider usage patterns
4. **Balance is key** - don't over-optimize, focus on biggest wins
5. **Test everything** - measure before and after!

---

## Support

For questions or issues, please refer to the ClickHouse community forums or documentation.

**Last Updated:** October 2025
